# Multi-Backend RAG System for Document Q&A

## 1. Project Description

This project is an intelligent query-retrieval system that answers natural language questions based on a collection of private documents. It implements a Retrieval-Augmented Generation (RAG) pipeline using LlamaIndex.

A key feature of this system is its resilient, multi-backend LLM setup. It ensures high availability by automatically falling back to secondary models if the primary model fails, providing a robust and reliable service.

## 2. Tech Stack

* **Core Framework**: LlamaIndex
* **Programming Language**: Python
* **LLM Providers**: Google Gemini, Local models via Ollama
* **Embedding Model**: BAAI/bge-base-en-v1.5 (via HuggingFace)
* **Vector Storage**: Local file system (`./storage/`)
* **Python Libraries**: `python-dotenv`, `asyncio`

## 3. LLM Fallback Logic

The system is configured to query multiple Large Language Models in a prioritized sequence to ensure a response is always generated, even if one service is down. The fallback order is defined in the `.env` file.

The default logic is as follows:
1. **Attempt 1: Google Gemini**: The system first tries to get a response using the `gemini-1.5-flash` model via the Google Gemini API.
2. **Fallback to Local Models**: If the Gemini API call fails (due to an invalid key, network error, or server issue), the system automatically switches to using local models served by Ollama.
3. **Iterate Through Local Models**: The system will try each local model specified in the `LOCAL_LLM_MODELS` environment variable in order. For example, if configured for `llama3,mistral`:
   * It will first try **LLaMA 3**.
   * If LLaMA 3 fails, it will then try **Mistral**.
4. **Final Failure**: If all configured models (both cloud and local) fail, the program will raise an exception.

This entire process is handled asynchronously to maintain performance.

## 4. Folder Structure

The project is organized as follows:

```
.
├── data/
│   └── BAJAJHLIP23020V012223.pdf
│   └── ... (other PDF documents)
├── storage/
│   └── default__vector_store.json
│   └── ... (persisted index files)
├── .env
├── ingest.py             # Script to process documents and create the vector index
├── llm_config.py         # Configures and provides LLM and embedding models
├── main.py               # Main script to load the index and query it
├── requirements.txt      # Project dependencies
└── ... (other helper scripts and cache files)
```

## 5. Setup Instructions

### Step 1: Clone the Repository
```bash
git clone <your-repository-url>
cd <your-repository-name>
```

### Step 2: Install Dependencies

Ensure you have Python 3.9+ installed. Then, install the required packages.

```bash
pip install -r requirements.txt
```

### Step 3: Set Up Environment Variables

Create a file named `.env` in the root directory of the project and add the following variables.

```ini
# .env file

# Your Google Gemini API Key
GOOGLE_GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE"

# Define the priority of LLM providers. The script will try them in this order.
LLM_PRIORITY="gemini,local"

# Define the local models to try, in order. Must be served by Ollama.
LOCAL_LLM_MODELS="llama3,mistral"
```

### Step 4: Set Up Local LLMs with Ollama

1. Install [Ollama](https://ollama.com/) on your system.
2. Pull the required local models from the Ollama library.
   ```bash
   ollama pull llama3
   ollama pull mistral
   ```
3. Ensure the Ollama application is running in the background to serve the models.

## 6. How to Run the Project

The project runs in two stages:

### Stage 1: Ingest Documents

First, you need to process the documents in the `data/` folder and create a vector index. Run the `ingest.py` script for this.

```bash
python ingest.py
```

This will create a `storage/` directory containing the persisted index. You only need to run this once, or whenever the documents in `data/` change.

### Stage 2: Query the Documents

To ask a question, run the `main.py` script.

```bash
python main.py
```

This script will load the existing index, use the hardcoded test query, and utilize the fallback LLM logic to generate and print an answer.

## 7. Expected Input/Output

* **Input**: A natural language question (currently hardcoded in `main.py`).
* **Output**: A text-based answer generated by one of the LLMs, which is printed to the console. The console logs will also show which LLM provider was successfully used.
